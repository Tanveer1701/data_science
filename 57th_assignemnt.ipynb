{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016fdaa1-7a9b-48c8-86f7-746ae994b6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is an ensemble technique in machine learning?\n",
    "ans- an ensemble technique refers to the process of combining multiple individual models to create a stronger and more accurate predictive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd925346-b90b-426f-bbc4-f0376fb8d00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Why are ensemble techniques used in machine learning?\n",
    "ans-Improved Accuracy: Ensemble techniques have the potential to improve predictive accuracy compared to individual models. By combining the predictions of multiple models, ensemble techniques can reduce errors and biases inherent in any single model. Ensemble models often achieve better generalization and perform well on unseen data.\n",
    "\n",
    "Robustness and Stability: Ensemble techniques are known for their robustness and stability. They can reduce the impact of outliers or noisy data points by averaging or combining predictions from multiple models. Ensemble models are less prone to overfitting and are more reliable in handling variance in the data.\n",
    "\n",
    "Model Diversity: Ensemble techniques leverage the concept of model diversity, where each individual model may have different strengths, weaknesses, or biases. By combining diverse models, ensemble techniques can capture a broader range of patterns and relationships in the data, leading to more robust and accurate prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8111e1-0856-4d8b-a83f-2b6d7edecdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is bagging?\n",
    "ans-Bagging, short for Bootstrap Aggregating, is an ensemble technique in machine learning that aims to improve the accuracy and stability of a predictive model by combining the predictions of multiple independently trained models. Bagging is primarily used to reduce variance and minimize overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a59e0cb-557c-4bd2-86d1-d1437ecbfac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is boosting?\n",
    "ans-Boosting is an ensemble technique in machine learning that combines multiple weak or base models to create a strong predictive model. Unlike bagging, which trains models independently, boosting trains models sequentially in a stagewise manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe70777-0405-4db2-a9ac-26a7f86de72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. What are the benefits of using ensemble techniques?\n",
    "ans-Improved Accuracy: Ensemble techniques have the potential to improve predictive accuracy compared to individual models. By combining the predictions of multiple models, ensemble techniques can reduce errors and biases inherent in any single model. Ensemble models often achieve better generalization and perform well on unseen data.\n",
    "\n",
    "Robustness and Stability: Ensemble techniques are known for their robustness and stability. They can reduce the impact of outliers or noisy data points by averaging or combining predictions from multiple models. Ensemble models are less prone to overfitting and are more reliable in handling variance in the data.\n",
    "\n",
    "Model Diversity: Ensemble techniques leverage the concept of model diversity, where each individual model may have different strengths, weaknesses, or biases. By combining diverse models, ensemble techniques can capture a broader range of patterns and relationships in the data, leading to more robust and accurate prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51683b10-77b5-4459-b5bc-fdccf42e2298",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Are ensemble techniques always better than individual models?\n",
    "ans-Ensemble techniques are powerful and often outperform individual models in terms of predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43819321-8c99-40ec-a1df-b2c69499c8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. How is the confidence interval calculated using bootstrap?\n",
    "ans-Sampling with Replacement: The bootstrap method starts by resampling the original dataset with replacement to create multiple bootstrap samples. Each bootstrap sample has the same size as the original dataset but may contain duplicate instances and exclude some instances from the original dataset.\n",
    "\n",
    "Estimation: For each bootstrap sample, the parameter or statistic of interest (e.g., mean, median, etc.) is estimated. This could be done by applying the desired analysis or model to the bootstrap sample.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370f5cde-5502-4568-9719-e59a67c78e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. How does bootstrap work and What are the steps involved in bootstrap?\n",
    "ans-uncertainty of statistical estimates or parameters. It works by creating multiple bootstrap samples from the original dataset through sampling with replacement. Here are the steps involved in the bootstrap method:\n",
    "\n",
    "Step 1: Original Dataset: Start with the original dataset, which typically consists of observed data points or a sample from a population.\n",
    "\n",
    "Step 2: Resampling with Replacement: Randomly select data points from the original dataset with replacement to create a bootstrap sample. The size of the bootstrap sample is typically the same as the size of the original dataset. As sampling is done with replacement, some data points from the original dataset may be selected multiple times, while others may not be selected at all in a particular bootstrap sample.\n",
    "\n",
    "Step 3: Estimation: Perform the desired analysis or computation on each bootstrap sample to estimate the parameter or statistic of interest. This could involve applying a statistical model, calculating a summary statistic, or performing any relevant analysis. The estimation step is typically applied independently to each bootstrap sample.\n",
    "\n",
    "Step 4: Repeat Steps 2 and 3: Repeat steps 2 and 3 a large number of times (typically hundreds or thousands) to create multiple bootstrap samples and obtain a collection of bootstrap estimates for the parameter of interest. Each iteration involves resampling the original dataset and estimating the parameter or statistic.\n",
    "\n",
    "Step 5: Variability and Confidence Interval: Analyze the collection of bootstrap estimates to assess the variability and uncertainty associated with the parameter or statistic of interest. This could involve calculating measures of central tendency (e.g., mean, median) or dispersion (e.g., standard deviation, confidence interval) based on the collection of bootstrap estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556bce3e-e40a-4595-9fb3-588360ab98f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a\n",
    "sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use\n",
    "bootstrap to estimate the 95% confidence interval for the population mean height.\n",
    "ans-import numpy as np\n",
    "\n",
    "# Observed sample of tree heights\n",
    "observed_heights = np.array([15] * 50)\n",
    "\n",
    "# Number of bootstrap samples\n",
    "n_bootstrap = 1000\n",
    "\n",
    "# Generate bootstrap samples and calculate sample means\n",
    "bootstrap_means = []\n",
    "for _ in range(n_bootstrap):\n",
    "    bootstrap_sample = np.random.choice(observed_heights, size=len(observed_heights), replace=True)\n",
    "    bootstrap_mean = np.mean(bootstrap_sample)\n",
    "    bootstrap_means.append(bootstrap_mean)\n",
    "\n",
    "# Calculate confidence interval\n",
    "lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "# Report the results\n",
    "confidence_interval = (lower_bound, upper_bound)\n",
    "print(\"95% Confidence Interval:\", confidence_interval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970d64ba-ad76-4fbf-8aeb-0e06e7c217f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
