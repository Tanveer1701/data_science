{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48655dc1-cc9a-47fb-8470-68be82b4c510",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Gradient Boosting Regression?\n",
    "ans-Gradient Boosting Regression is a machine learning algorithm that belongs to the class of boosting algorithms. It is used for regression tasks, where the goal is to predict continuous numeric values rather than class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091a2840-87dc-4c8c-aeba-e993986bc685",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?\n",
    "ans-In the context of Gradient Boosting, a weak learner refers to a simple, base model that performs slightly better than random guessing or a model with very low complexity. Typically, weak learners in Gradient Boosting are decision trees with a small depth or few nodes, often referred to as \"decision stumps.\" Decision stumps are shallow decision trees that make predictions based on a single feature or a small subset of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc637d8-8a51-42e2-8105-a3d4c45f2e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "ans-The intuition behind the Gradient Boosting algorithm is to iteratively build a strong predictive model by combining multiple weak models. The algorithm works in a sequential manner, where each weak model is trained to correct the mistakes made by the previous models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbc9e22-5cae-445b-9a09-81106bfb310d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "ans-The Gradient Boosting algorithm builds an ensemble of weak learners in a sequential and iterative manner. Here are the key steps involved in building the ensemble:\n",
    "\n",
    "Initialization: The ensemble is initialized with a simple model, often a constant value or the mean of the target variable. This initial model serves as the baseline prediction.\n",
    "\n",
    "Compute Pseudo-Residuals: Pseudo-residuals are calculated by taking the negative gradient of a chosen loss function with respect to the current ensemble's predictions. The pseudo-residuals represent the errors or the portion of the target variable that is not yet captured by the ensemble.\n",
    "\n",
    "Train a Weak Learner: A weak learner, usually a decision tree with a small depth or few nodes, is trained to predict the pseudo-residuals. The weak learner aims to find patterns or relationships in the data that help reduce the pseudo-residuals. The weak learner is fitted to the training data using the features and the computed pseudo-residuals.\n",
    "\n",
    "Update Ensemble Predictions: The predictions made by the weak learner are multiplied by a small learning rate (also known as the shrinkage factor) and added to the ensemble's predictions. The learning rate controls the contribution of each weak learner to the final ensemble. By updating the ensemble predictions with the weighted predictions from the weak learner, the ensemble gradually improves its overall performance.\n",
    "\n",
    "Repeat Steps 2-4: Steps 2 to 4 are repeated for a specified number of iterations or until a stopping criterion is met. In each iteration, new pseudo-residuals are computed based on the current ensemble's predictions, a new weak learner is trained to predict these pseudo-residuals, and the ensemble predictions are updated accordingly.\n",
    "\n",
    "Final Ensemble Prediction: The final ensemble prediction is obtained by summing up the predictions of all the weak learners in the ensemble. Each weak learner's prediction is multiplied by its corresponding learning rate before being combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93148a1-f291-4316-b0cc-f07f2a3dc856",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?\n",
    "ans-\n",
    "The mathematical intuition behind the Gradient Boosting algorithm involves the following steps:\n",
    "\n",
    "Loss Function Selection: Choose a suitable loss function that quantifies the difference between the predicted values and the actual target values. Commonly used loss functions for regression problems include mean squared error (MSE) and mean absolute error (MAE).\n",
    "\n",
    "Initialization of Ensemble: Initialize the ensemble by setting the initial predictions to a constant value or the mean of the target variable. This serves as the starting point for the iterative process.\n",
    "\n",
    "Compute Pseudo-Residuals: Compute the pseudo-residuals, which represent the negative gradient of the chosen loss function with respect to the current ensemble's predictions. Pseudo-residuals capture the errors or the portion of the target variable that is not yet captured by the ensemble.\n",
    "\n",
    "Train Weak Learner on Pseudo-Residuals: Fit a weak learner, such as a decision tree with limited depth or few nodes, to the pseudo-residuals. The weak learner aims to find patterns in the data that can help reduce the pseudo-residuals. The weak learner is trained using the features and the computed pseudo-residuals.\n",
    "\n",
    "Update Ensemble Predictions: Multiply the predictions made by the weak learner by a small learning rate (shrinkage factor) and add them to the ensemble's predictions. The learning rate controls the contribution of each weak learner to the final ensemble. By updating the ensemble predictions with the weighted predictions from the weak learner, the ensemble gradually improves its overall performance.\n",
    "\n",
    "Update Pseudo-Residuals: Recalculate the pseudo-residuals based on the difference between the actual target values and the updated ensemble predictions. The updated pseudo-residuals capture the errors that remain after incorporating the predictions from the weak learner.\n",
    "\n",
    "Repeat Steps 4-6: Repeat steps 4 to 6 for a specified number of iterations or until a stopping criterion is met. In each iteration, a new weak learner is trained on the updated pseudo-residuals, and the ensemble predictions and pseudo-residuals are updated accordingly.\n",
    "\n",
    "Final Ensemble Prediction: Obtain the final ensemble prediction by summing up the predictions of all the weak learners in the ensemble. Each weak learner's prediction is multiplied by its corresponding learning rate before being combined."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
