{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2023-07-05T09:09:37.061578Z","iopub.execute_input":"2023-07-05T09:09:37.061937Z","iopub.status.idle":"2023-07-05T09:09:37.069565Z","shell.execute_reply.started":"2023-07-05T09:09:37.061906Z","shell.execute_reply":"2023-07-05T09:09:37.068552Z"},"trusted":true},"execution_count":118,"outputs":[{"name":"stdout","text":"/kaggle/input/sentiment-prediction-on-movie-reviews/movies.csv\n/kaggle/input/sentiment-prediction-on-movie-reviews/sample.csv\n/kaggle/input/sentiment-prediction-on-movie-reviews/train.csv\n/kaggle/input/sentiment-prediction-on-movie-reviews/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"#import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport string\nimport re","metadata":{"execution":{"iopub.status.busy":"2023-07-05T09:09:37.071911Z","iopub.execute_input":"2023-07-05T09:09:37.072333Z","iopub.status.idle":"2023-07-05T09:09:37.088812Z","shell.execute_reply.started":"2023-07-05T09:09:37.072302Z","shell.execute_reply":"2023-07-05T09:09:37.087569Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"code","source":"df_train=pd.read_csv(\"/kaggle/input/sentiment-prediction-on-movie-reviews/train.csv\")\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-05T09:09:37.090189Z","iopub.execute_input":"2023-07-05T09:09:37.090912Z","iopub.status.idle":"2023-07-05T09:09:37.488459Z","shell.execute_reply.started":"2023-07-05T09:09:37.090885Z","shell.execute_reply":"2023-07-05T09:09:37.487390Z"},"trusted":true},"execution_count":120,"outputs":[{"execution_count":120,"output_type":"execute_result","data":{"text/plain":"                                             movieid         reviewerName  \\\n0                                   marvelous_pirate       Benjamin Henry   \n1          tony_montana_frodo_baggins_v_rocky_balboa        Felicia Lopez   \n2  darth_vader_katniss_everdeen_sorcerer_donnie_d...  Mr. Charles Burgess   \n3                                 lara_croft_glimmer         Ryan Barrett   \n4  jason_bourne_surreal_the_terminator_indiana_jones     Alexander Glover   \n\n   isFrequentReviewer                                         reviewText  \\\n0               False  Henry Selick’s first movie since 2009’s Corali...   \n1               False  With a cast that reads like the Vogue Oscar pa...   \n2                True  Creed II does not give us anything but another...   \n3               False  I know what you're thinking, but this is no Li...   \n4               False  Director Fernando Meirelles tells the story wi...   \n\n  sentiment  \n0  POSITIVE  \n1  NEGATIVE  \n2  POSITIVE  \n3  POSITIVE  \n4  POSITIVE  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>movieid</th>\n      <th>reviewerName</th>\n      <th>isFrequentReviewer</th>\n      <th>reviewText</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>marvelous_pirate</td>\n      <td>Benjamin Henry</td>\n      <td>False</td>\n      <td>Henry Selick’s first movie since 2009’s Corali...</td>\n      <td>POSITIVE</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>tony_montana_frodo_baggins_v_rocky_balboa</td>\n      <td>Felicia Lopez</td>\n      <td>False</td>\n      <td>With a cast that reads like the Vogue Oscar pa...</td>\n      <td>NEGATIVE</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>darth_vader_katniss_everdeen_sorcerer_donnie_d...</td>\n      <td>Mr. Charles Burgess</td>\n      <td>True</td>\n      <td>Creed II does not give us anything but another...</td>\n      <td>POSITIVE</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>lara_croft_glimmer</td>\n      <td>Ryan Barrett</td>\n      <td>False</td>\n      <td>I know what you're thinking, but this is no Li...</td>\n      <td>POSITIVE</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>jason_bourne_surreal_the_terminator_indiana_jones</td>\n      <td>Alexander Glover</td>\n      <td>False</td>\n      <td>Director Fernando Meirelles tells the story wi...</td>\n      <td>POSITIVE</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#drop first 3 columns\ndf_train=df_train.drop(['movieid','reviewerName','isFrequentReviewer'],axis=1)\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-05T09:09:37.490279Z","iopub.execute_input":"2023-07-05T09:09:37.490670Z","iopub.status.idle":"2023-07-05T09:09:37.506164Z","shell.execute_reply.started":"2023-07-05T09:09:37.490637Z","shell.execute_reply":"2023-07-05T09:09:37.505136Z"},"trusted":true},"execution_count":121,"outputs":[{"execution_count":121,"output_type":"execute_result","data":{"text/plain":"                                          reviewText sentiment\n0  Henry Selick’s first movie since 2009’s Corali...  POSITIVE\n1  With a cast that reads like the Vogue Oscar pa...  NEGATIVE\n2  Creed II does not give us anything but another...  POSITIVE\n3  I know what you're thinking, but this is no Li...  POSITIVE\n4  Director Fernando Meirelles tells the story wi...  POSITIVE","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reviewText</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Henry Selick’s first movie since 2009’s Corali...</td>\n      <td>POSITIVE</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>With a cast that reads like the Vogue Oscar pa...</td>\n      <td>NEGATIVE</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Creed II does not give us anything but another...</td>\n      <td>POSITIVE</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>I know what you're thinking, but this is no Li...</td>\n      <td>POSITIVE</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Director Fernando Meirelles tells the story wi...</td>\n      <td>POSITIVE</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#fill missing values with blank\ndf_train=df_train.fillna(\" \")\ndf_train.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-07-05T09:09:37.507749Z","iopub.execute_input":"2023-07-05T09:09:37.508115Z","iopub.status.idle":"2023-07-05T09:09:37.637087Z","shell.execute_reply.started":"2023-07-05T09:09:37.508082Z","shell.execute_reply":"2023-07-05T09:09:37.635986Z"},"trusted":true},"execution_count":122,"outputs":[{"execution_count":122,"output_type":"execute_result","data":{"text/plain":"reviewText    0\nsentiment     0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"#editing reviewText column\n\n# Lowercasing\ndf_train['reviewText'] = df_train['reviewText'].str.lower()\n\n# Removing Punctuation\ndf_train['reviewText'] = df_train['reviewText'].apply(lambda text: re.sub(r'[^\\w\\s]', '', text))\n\n# Tokenization\ndf_train['reviewText'] = df_train['reviewText'].apply(lambda text: re.findall(r'\\w+', text))\n\n# Removing Stop Words\nstop_words = set(['the', 'is', 'and'])  # Add more stop words if needed\ndf_train['reviewText'] = df_train['reviewText'].apply(lambda tokens: [word for word in tokens if word not in stop_words])\n\n\n# Removing Numbers\ndf_train['reviewText'] = df_train['reviewText'].apply(lambda tokens: [word for word in tokens if not word.isdigit()])\n\n# Removing Extra Whitespaces\ndf_train['reviewText'] = df_train['reviewText'].apply(lambda tokens: [re.sub(r'\\s+', ' ', word) for word in tokens])\n\n# Joining Tokens back to Text\ndf_train['reviewText'] = df_train['reviewText'].apply(' '.join)\n\n# Handling Special Characters or URLs (Example: Removing URLs)\ndf_train['reviewText'] = df_train['reviewText'].apply(lambda text: re.sub(r'http\\S+', '', text))\n","metadata":{"execution":{"iopub.status.busy":"2023-07-05T09:09:37.639852Z","iopub.execute_input":"2023-07-05T09:09:37.640263Z","iopub.status.idle":"2023-07-05T09:09:45.578879Z","shell.execute_reply.started":"2023-07-05T09:09:37.640212Z","shell.execute_reply":"2023-07-05T09:09:45.577623Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"code","source":"#selecting X and y \nX=df_train['reviewText']\ny=df_train['sentiment']\n\n","metadata":{"execution":{"iopub.status.busy":"2023-07-05T09:09:45.580380Z","iopub.execute_input":"2023-07-05T09:09:45.580790Z","iopub.status.idle":"2023-07-05T09:09:45.589365Z","shell.execute_reply.started":"2023-07-05T09:09:45.580754Z","shell.execute_reply":"2023-07-05T09:09:45.588135Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"code","source":"#import necessary sklearn modules\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","metadata":{"execution":{"iopub.status.busy":"2023-07-05T09:09:45.590364Z","iopub.execute_input":"2023-07-05T09:09:45.591187Z","iopub.status.idle":"2023-07-05T09:09:45.603012Z","shell.execute_reply.started":"2023-07-05T09:09:45.591161Z","shell.execute_reply":"2023-07-05T09:09:45.601739Z"},"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"code","source":"#train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-07-05T09:09:45.604801Z","iopub.execute_input":"2023-07-05T09:09:45.605105Z","iopub.status.idle":"2023-07-05T09:09:45.664852Z","shell.execute_reply.started":"2023-07-05T09:09:45.605080Z","shell.execute_reply":"2023-07-05T09:09:45.663845Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"code","source":"#vectorize the text using count vectorizer\nvectorizer=TfidfVectorizer()\nX_train_vec=vectorizer.fit_transform(X_train)\nX_test_vec=vectorizer.transform(X_test)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-07-05T09:09:45.666596Z","iopub.execute_input":"2023-07-05T09:09:45.666969Z","iopub.status.idle":"2023-07-05T09:09:48.815428Z","shell.execute_reply.started":"2023-07-05T09:09:45.666942Z","shell.execute_reply":"2023-07-05T09:09:48.814163Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"code","source":"#Train Logistic Model\nmodel=LogisticRegression()\nmodel.fit(X_train_vec,y_train)","metadata":{"execution":{"iopub.status.busy":"2023-07-05T09:09:48.816537Z","iopub.execute_input":"2023-07-05T09:09:48.816801Z","iopub.status.idle":"2023-07-05T09:09:54.440661Z","shell.execute_reply.started":"2023-07-05T09:09:48.816778Z","shell.execute_reply":"2023-07-05T09:09:54.439880Z"},"trusted":true},"execution_count":128,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"execution_count":128,"output_type":"execute_result","data":{"text/plain":"LogisticRegression()","text/html":"<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"code","source":"#make predictions on test set\ny_pred=model.predict(X_test_vec)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-05T09:09:54.444137Z","iopub.execute_input":"2023-07-05T09:09:54.444620Z","iopub.status.idle":"2023-07-05T09:09:54.455595Z","shell.execute_reply.started":"2023-07-05T09:09:54.444594Z","shell.execute_reply":"2023-07-05T09:09:54.454717Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"code","source":"#calculate accuracy of the model\naccuracy=accuracy_score(y_test,y_pred)\nprint(\"Accuracy:\",accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-07-05T09:09:54.457369Z","iopub.execute_input":"2023-07-05T09:09:54.458580Z","iopub.status.idle":"2023-07-05T09:09:54.564265Z","shell.execute_reply.started":"2023-07-05T09:09:54.458552Z","shell.execute_reply":"2023-07-05T09:09:54.562451Z"},"trusted":true},"execution_count":130,"outputs":[{"name":"stdout","text":"Accuracy: 0.7970324404030474\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import cross_val_score\n\n# Split the dataset into dependent(X) and independent variable(y)\nX = df_train['reviewText']\ny = df_train['sentiment']\n\n# Split the dataset into training and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Construct a pipeline for text classification\npipeline = Pipeline([\n    ('vect', TfidfVectorizer()),\n    ('clf', LinearSVC())\n])\n\n# Set the parameters for the pipeline\nparameters = {\n    'vect__ngram_range': [(1, 1), (1, 2)],  # Try both unigrams and bigrams\n    'clf__C': [0.1, 1, 10]\n}\n\n# Perform randomized search to find the best parameters\nrandom_search = RandomizedSearchCV(pipeline, parameters, cv=5, n_iter=3)\nrandom_search.fit(X_train, y_train)\n\n# Get the best model\nbest_model = random_search.best_estimator_\n\n# Make predictions on the test set\ny_pred = best_model.predict(X_test)\n\n# Generate the classification report\nreport = classification_report(y_test, y_pred)\nprint('Classification Report:')\nprint(report)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-05T09:09:54.565542Z","iopub.execute_input":"2023-07-05T09:09:54.565779Z","iopub.status.idle":"2023-07-05T09:11:42.301717Z","shell.execute_reply.started":"2023-07-05T09:09:54.565759Z","shell.execute_reply":"2023-07-05T09:11:42.300162Z"},"trusted":true},"execution_count":131,"outputs":[{"name":"stdout","text":"Classification Report:\n              precision    recall  f1-score   support\n\n    NEGATIVE       0.78      0.54      0.64     10696\n    POSITIVE       0.80      0.93      0.86     21856\n\n    accuracy                           0.80     32552\n   macro avg       0.79      0.73      0.75     32552\nweighted avg       0.80      0.80      0.79     32552\n\n","output_type":"stream"}]},{"cell_type":"code","source":"\"\"\"\n\n#Naive Bayes\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import cross_val_score\n\n# Split the dataset into dependent (X) and independent variable (y)\nX = df_train['reviewText']\ny = df_train['sentiment']\n\n# Split the dataset into training and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Construct a pipeline for text classification\npipeline = Pipeline([\n    ('vect', TfidfVectorizer()),\n    ('clf', MultinomialNB())\n])\n\n# Set the parameters for the pipeline\nparameters = {\n    'vect__ngram_range': [(1, 1), (1, 2)],  # Try both unigrams and bigrams\n    'clf__alpha': [0.1, 1, 10]  # Try different values for the smoothing parameter alpha\n}\n\n# Perform randomized search to find the best parameters\nrandom_search = RandomizedSearchCV(pipeline, parameters, cv=5, n_iter=3)\nrandom_search.fit(X_train, y_train)\n\n# Get the best model\nbest_model = random_search.best_estimator_\n\n# Evaluate the model\ny_pred = best_model.predict(X_test)\nreport = classification_report(y_test, y_pred)\nprint(report)\n\"\"\"\n","metadata":{"execution":{"iopub.status.busy":"2023-07-05T09:11:42.303024Z","iopub.execute_input":"2023-07-05T09:11:42.303399Z","iopub.status.idle":"2023-07-05T09:11:42.311666Z","shell.execute_reply.started":"2023-07-05T09:11:42.303374Z","shell.execute_reply":"2023-07-05T09:11:42.310296Z"},"trusted":true},"execution_count":132,"outputs":[{"execution_count":132,"output_type":"execute_result","data":{"text/plain":"\"\\n\\n#Naive Bayes\\n\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.naive_bayes import MultinomialNB\\nfrom sklearn.metrics import classification_report\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.model_selection import RandomizedSearchCV\\nfrom sklearn.model_selection import cross_val_score\\n\\n# Split the dataset into dependent (X) and independent variable (y)\\nX = df_train['reviewText']\\ny = df_train['sentiment']\\n\\n# Split the dataset into training and test set\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Construct a pipeline for text classification\\npipeline = Pipeline([\\n    ('vect', TfidfVectorizer()),\\n    ('clf', MultinomialNB())\\n])\\n\\n# Set the parameters for the pipeline\\nparameters = {\\n    'vect__ngram_range': [(1, 1), (1, 2)],  # Try both unigrams and bigrams\\n    'clf__alpha': [0.1, 1, 10]  # Try different values for the smoothing parameter alpha\\n}\\n\\n# Perform randomized search to find the best parameters\\nrandom_search = RandomizedSearchCV(pipeline, parameters, cv=5, n_iter=3)\\nrandom_search.fit(X_train, y_train)\\n\\n# Get the best model\\nbest_model = random_search.best_estimator_\\n\\n# Evaluate the model\\ny_pred = best_model.predict(X_test)\\nreport = classification_report(y_test, y_pred)\\nprint(report)\\n\""},"metadata":{}}]},{"cell_type":"code","source":"\"\"\"\n#decision tree\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Split the dataset into dependent (X) and independent variable (y)\nX = df_train['reviewText']\ny = df_train['sentiment']\n\n# Split the dataset into training and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Construct a pipeline for text classification\npipeline = Pipeline([\n    ('vect', TfidfVectorizer()),\n    ('clf', DecisionTreeClassifier())\n])\n\n# Set the parameters for the pipeline\nparameters = {\n    'vect__ngram_range': [(1, 1), (1, 2)],  # Try both unigrams and bigrams\n    'clf__max_depth': [None, 10, 20]  # Try different values for the maximum depth\n}\n\n# Perform randomized search to find the best parameters\nrandom_search = RandomizedSearchCV(pipeline, parameters, cv=5, n_iter=3)\nrandom_search.fit(X_train, y_train)\n\n# Get the best model\nbest_model = random_search.best_estimator_\n\n# Make predictions on the test set\ny_pred = best_model.predict(X_test)\n\n# Generate the classification report\nreport = classification_report(y_test, y_pred)\nprint(report)\n\"\"\"\n","metadata":{"execution":{"iopub.status.busy":"2023-07-05T09:11:42.313199Z","iopub.execute_input":"2023-07-05T09:11:42.313800Z","iopub.status.idle":"2023-07-05T09:11:42.331989Z","shell.execute_reply.started":"2023-07-05T09:11:42.313767Z","shell.execute_reply":"2023-07-05T09:11:42.330460Z"},"trusted":true},"execution_count":133,"outputs":[{"execution_count":133,"output_type":"execute_result","data":{"text/plain":"\"\\n#decision tree\\n\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.metrics import classification_report\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.model_selection import RandomizedSearchCV\\n\\n# Split the dataset into dependent (X) and independent variable (y)\\nX = df_train['reviewText']\\ny = df_train['sentiment']\\n\\n# Split the dataset into training and test set\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Construct a pipeline for text classification\\npipeline = Pipeline([\\n    ('vect', TfidfVectorizer()),\\n    ('clf', DecisionTreeClassifier())\\n])\\n\\n# Set the parameters for the pipeline\\nparameters = {\\n    'vect__ngram_range': [(1, 1), (1, 2)],  # Try both unigrams and bigrams\\n    'clf__max_depth': [None, 10, 20]  # Try different values for the maximum depth\\n}\\n\\n# Perform randomized search to find the best parameters\\nrandom_search = RandomizedSearchCV(pipeline, parameters, cv=5, n_iter=3)\\nrandom_search.fit(X_train, y_train)\\n\\n# Get the best model\\nbest_model = random_search.best_estimator_\\n\\n# Make predictions on the test set\\ny_pred = best_model.predict(X_test)\\n\\n# Generate the classification report\\nreport = classification_report(y_test, y_pred)\\nprint(report)\\n\""},"metadata":{}}]},{"cell_type":"code","source":"\"\"\"\n#decision tree\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Split the dataset into dependent (X) and independent variable (y)\nX = df_train['reviewText']\ny = df_train['sentiment']\n\n# Split the dataset into training and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Construct a pipeline for text classification\npipeline = Pipeline([\n    ('vect', TfidfVectorizer()),\n    ('clf', DecisionTreeClassifier())\n])\n\n# Set the parameters for the pipeline\nparameters = {\n    'vect__ngram_range': [(1, 1), (1, 2)],  # Try both unigrams and bigrams\n    'clf__max_depth': [None, 10, 20]  # Try different values for the maximum depth\n}\n\n# Perform randomized search to find the best parameters\nrandom_search = RandomizedSearchCV(pipeline, parameters, cv=5, n_iter=3)\nrandom_search.fit(X_train, y_train)\n\n# Get the best model\nbest_model = random_search.best_estimator_\n\n# Make predictions on the test set\ny_pred = best_model.predict(X_test)\n\n# Generate the classification report\nreport = classification_report(y_test, y_pred)\nprint(report)\n\"\"\"\n","metadata":{"execution":{"iopub.status.busy":"2023-07-05T09:11:42.334290Z","iopub.execute_input":"2023-07-05T09:11:42.334697Z","iopub.status.idle":"2023-07-05T09:11:42.352572Z","shell.execute_reply.started":"2023-07-05T09:11:42.334667Z","shell.execute_reply":"2023-07-05T09:11:42.351065Z"},"trusted":true},"execution_count":134,"outputs":[{"execution_count":134,"output_type":"execute_result","data":{"text/plain":"\"\\n#decision tree\\n\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.metrics import classification_report\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.model_selection import RandomizedSearchCV\\n\\n# Split the dataset into dependent (X) and independent variable (y)\\nX = df_train['reviewText']\\ny = df_train['sentiment']\\n\\n# Split the dataset into training and test set\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Construct a pipeline for text classification\\npipeline = Pipeline([\\n    ('vect', TfidfVectorizer()),\\n    ('clf', DecisionTreeClassifier())\\n])\\n\\n# Set the parameters for the pipeline\\nparameters = {\\n    'vect__ngram_range': [(1, 1), (1, 2)],  # Try both unigrams and bigrams\\n    'clf__max_depth': [None, 10, 20]  # Try different values for the maximum depth\\n}\\n\\n# Perform randomized search to find the best parameters\\nrandom_search = RandomizedSearchCV(pipeline, parameters, cv=5, n_iter=3)\\nrandom_search.fit(X_train, y_train)\\n\\n# Get the best model\\nbest_model = random_search.best_estimator_\\n\\n# Make predictions on the test set\\ny_pred = best_model.predict(X_test)\\n\\n# Generate the classification report\\nreport = classification_report(y_test, y_pred)\\nprint(report)\\n\""},"metadata":{}}]},{"cell_type":"code","source":"#load the test file\n\ndf_test=pd.read_csv(\"/kaggle/input/sentiment-prediction-on-movie-reviews/test.csv\")\ndf_test.head()\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-07-05T09:11:42.353534Z","iopub.execute_input":"2023-07-05T09:11:42.353825Z","iopub.status.idle":"2023-07-05T09:11:42.500977Z","shell.execute_reply.started":"2023-07-05T09:11:42.353802Z","shell.execute_reply":"2023-07-05T09:11:42.499976Z"},"trusted":true},"execution_count":135,"outputs":[{"execution_count":135,"output_type":"execute_result","data":{"text/plain":"                               movieid     reviewerName  isTopCritic  \\\n0            legend_marty_mcfly_oracle         John Kim        False   \n1  terminator_katniss_everdeen_glimmer     Brian Chaney        False   \n2          james_bond_labyrinth_gollum  Danielle Parker        False   \n3            v_quest_han_solo_wondrous    Brittany Lane        False   \n4        enigma_hulk_surreal_starlight    Justin Willis        False   \n\n                                          reviewText  \n0  Green slowly cranks up the dread with style an...  \n1  Philip Noyce's direction is elegant and unforc...  \n2  It wouldn't do to say what path Maria ultimate...  \n3  Pig is not exactly the arthouse John Wick that...  \n4  An imaginative no-budget musical of sorts abou...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>movieid</th>\n      <th>reviewerName</th>\n      <th>isTopCritic</th>\n      <th>reviewText</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>legend_marty_mcfly_oracle</td>\n      <td>John Kim</td>\n      <td>False</td>\n      <td>Green slowly cranks up the dread with style an...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>terminator_katniss_everdeen_glimmer</td>\n      <td>Brian Chaney</td>\n      <td>False</td>\n      <td>Philip Noyce's direction is elegant and unforc...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>james_bond_labyrinth_gollum</td>\n      <td>Danielle Parker</td>\n      <td>False</td>\n      <td>It wouldn't do to say what path Maria ultimate...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>v_quest_han_solo_wondrous</td>\n      <td>Brittany Lane</td>\n      <td>False</td>\n      <td>Pig is not exactly the arthouse John Wick that...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>enigma_hulk_surreal_starlight</td>\n      <td>Justin Willis</td>\n      <td>False</td>\n      <td>An imaginative no-budget musical of sorts abou...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#drop first 3 columns\ndf_test=df_test.drop(['movieid','reviewerName','isTopCritic'],axis=1)\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-05T09:11:42.503859Z","iopub.execute_input":"2023-07-05T09:11:42.504124Z","iopub.status.idle":"2023-07-05T09:11:42.515494Z","shell.execute_reply.started":"2023-07-05T09:11:42.504103Z","shell.execute_reply":"2023-07-05T09:11:42.514608Z"},"trusted":true},"execution_count":136,"outputs":[{"execution_count":136,"output_type":"execute_result","data":{"text/plain":"                                          reviewText\n0  Green slowly cranks up the dread with style an...\n1  Philip Noyce's direction is elegant and unforc...\n2  It wouldn't do to say what path Maria ultimate...\n3  Pig is not exactly the arthouse John Wick that...\n4  An imaginative no-budget musical of sorts abou...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reviewText</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Green slowly cranks up the dread with style an...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Philip Noyce's direction is elegant and unforc...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>It wouldn't do to say what path Maria ultimate...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Pig is not exactly the arthouse John Wick that...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>An imaginative no-budget musical of sorts abou...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#filling null values with blank\ndf_test['reviewText']=df_test['reviewText'].fillna(\" \")","metadata":{"execution":{"iopub.status.busy":"2023-07-05T09:11:42.517119Z","iopub.execute_input":"2023-07-05T09:11:42.517530Z","iopub.status.idle":"2023-07-05T09:11:42.533679Z","shell.execute_reply.started":"2023-07-05T09:11:42.517500Z","shell.execute_reply":"2023-07-05T09:11:42.532729Z"},"trusted":true},"execution_count":137,"outputs":[]},{"cell_type":"code","source":"#vectorize the test data using the same Countvectorizer used for training\nX_test_vectorized=vectorizer.transform(df_test['reviewText'])\n","metadata":{"execution":{"iopub.status.busy":"2023-07-05T09:11:42.535123Z","iopub.execute_input":"2023-07-05T09:11:42.535446Z","iopub.status.idle":"2023-07-05T09:11:43.613701Z","shell.execute_reply.started":"2023-07-05T09:11:42.535422Z","shell.execute_reply":"2023-07-05T09:11:43.612658Z"},"trusted":true},"execution_count":138,"outputs":[]},{"cell_type":"code","source":"#make prediction on the test data\ntest_predictions=best_model.predict(X_test_vectorized)\ntest_predictions\n","metadata":{"execution":{"iopub.status.busy":"2023-07-05T09:11:43.615460Z","iopub.execute_input":"2023-07-05T09:11:43.615890Z","iopub.status.idle":"2023-07-05T09:11:43.701040Z","shell.execute_reply.started":"2023-07-05T09:11:43.615853Z","shell.execute_reply":"2023-07-05T09:11:43.699444Z"},"trusted":true},"execution_count":139,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[139], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#make prediction on the test data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m test_predictions\u001b[38;5;241m=\u001b[39m\u001b[43mbest_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_vectorized\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m test_predictions\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/pipeline.py:480\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[0;34m(self, X, **predict_params)\u001b[0m\n\u001b[1;32m    478\u001b[0m Xt \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(with_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 480\u001b[0m     Xt \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpredict_params)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:2157\u001b[0m, in \u001b[0;36mTfidfVectorizer.transform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   2140\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Transform documents to document-term matrix.\u001b[39;00m\n\u001b[1;32m   2141\u001b[0m \n\u001b[1;32m   2142\u001b[0m \u001b[38;5;124;03mUses the vocabulary and document frequencies (df) learned by fit (or\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2153\u001b[0m \u001b[38;5;124;03m    Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[1;32m   2154\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2155\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m, msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe TF-IDF vectorizer is not fitted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2157\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mtransform(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1433\u001b[0m, in \u001b[0;36mCountVectorizer.transform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1430\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_vocabulary()\n\u001b[1;32m   1432\u001b[0m \u001b[38;5;66;03m# use the same matrix-building strategy as fit_transform\u001b[39;00m\n\u001b[0;32m-> 1433\u001b[0m _, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixed_vocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1435\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1275\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1273\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[1;32m   1274\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1275\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1276\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1277\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:111\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:69\u001b[0m, in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03mapply to a document.\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m    preprocessed string\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[0;32m---> 69\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m     doc \u001b[38;5;241m=\u001b[39m accent_function(doc)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/scipy/sparse/_base.py:771\u001b[0m, in \u001b[0;36mspmatrix.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetnnz()\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 771\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(attr \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mAttributeError\u001b[0m: lower not found"],"ename":"AttributeError","evalue":"lower not found","output_type":"error"}]},{"cell_type":"code","source":"#making a submission\nsubmission=pd.DataFrame(columns=['id','sentiment'])\nsubmission['id']=[i for i in range(len(test_predictions))]\nsubmission['sentiment']=test_predictions","metadata":{"execution":{"iopub.status.busy":"2023-07-05T09:11:43.701874Z","iopub.status.idle":"2023-07-05T09:11:43.702216Z","shell.execute_reply.started":"2023-07-05T09:11:43.702064Z","shell.execute_reply":"2023-07-05T09:11:43.702079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.shape\n","metadata":{"execution":{"iopub.status.busy":"2023-07-05T09:11:43.703486Z","iopub.status.idle":"2023-07-05T09:11:43.703796Z","shell.execute_reply.started":"2023-07-05T09:11:43.703650Z","shell.execute_reply":"2023-07-05T09:11:43.703664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.info","metadata":{"execution":{"iopub.status.busy":"2023-07-05T09:11:43.704821Z","iopub.status.idle":"2023-07-05T09:11:43.705105Z","shell.execute_reply.started":"2023-07-05T09:11:43.704966Z","shell.execute_reply":"2023-07-05T09:11:43.704980Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-05T09:11:43.706033Z","iopub.status.idle":"2023-07-05T09:11:43.706457Z","shell.execute_reply.started":"2023-07-05T09:11:43.706246Z","shell.execute_reply":"2023-07-05T09:11:43.706269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#save the submission file\nsubmission.to_csv(\"submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2023-07-05T09:11:43.708343Z","iopub.status.idle":"2023-07-05T09:11:43.708723Z","shell.execute_reply.started":"2023-07-05T09:11:43.708532Z","shell.execute_reply":"2023-07-05T09:11:43.708551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}