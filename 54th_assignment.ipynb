{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d5ca14-0c65-4bb0-a6f7-777a5a6ff436",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is a contingency matrix, and how is it used to evaluate the performance of a classification model?\n",
    "ans-A contingency matrix, also known as a confusion matrix, is a tabular representation that summarizes the performance of a classification model. It compares the predicted labels of the model with the actual labels of the data.\n",
    "\n",
    "The contingency matrix is typically a square matrix with rows and columns representing the true and predicted labels, respectively. Each cell in the matrix corresponds to the count or frequency of data points that fall into a particular combination of true and predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29f2aec-89e1-4b19-82ed-c5b3c8783084",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. How is a pair confusion matrix different from a regular confusion matrix, and why might it be useful in\n",
    "certain situations?\n",
    "ans-A pair confusion matrix, also known as an error matrix or cost matrix, is a variation of the regular confusion matrix that incorporates the concept of costs or weights associated with misclassification errors. While a regular confusion matrix focuses on the count or frequency of true and predicted labels, a pair confusion matrix considers the impact or consequences of misclassifications in terms of costs or weights assigned to different types of errors.\n",
    "\n",
    "In a pair confusion matrix, the rows represent the true labels, and the columns represent the predicted labels. Each cell in the matrix contains the associated cost or weight of misclassifying a data point from the true label to the predicted label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbffed8e-6990-488c-956b-9b964eaab479",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is an extrinsic measure in the context of natural language processing, and how is it typically\n",
    "used to evaluate the performance of language models?\n",
    "ans-In the context of natural language processing (NLP), an extrinsic measure refers to the evaluation of a language model's performance by assessing its effectiveness in solving a specific downstream task. It involves measuring the model's performance on a task that is directly relevant to its intended application, rather than evaluating the model based on intrinsic measures such as language fluency or coherence.\n",
    "\n",
    "Extrinsic evaluation is typically used to assess the practical utility of a language model in real-world scenarios. Instead of solely relying on intrinsic measures, which evaluate the model's internal properties, extrinsic evaluation focuses on the model's ability to perform specific tasks that require language understanding, generation, or processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fc8fd5-c4ba-4901-85ef-2e8506f1828e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is an intrinsic measure in the context of machine learning, and how does it differ from an\n",
    "extrinsic measure?\n",
    "ans-In the context of machine learning, intrinsic measures refer to the evaluation of a model's performance based on its internal properties or characteristics. These measures assess the model's performance independent of any specific application or task. In contrast, extrinsic measures evaluate the model's performance by measuring its effectiveness in solving a specific downstream task.\n",
    "\n",
    "Intrinsic measures focus on evaluating the model's inherent capabilities, such as its ability to learn patterns, generalize from data, and capture relevant features. These measures often involve assessing the model's performance on well-defined benchmark datasets and commonly used evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2f3b56-7ec2-4679-bc21-7b2f17c3a07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. What is the purpose of a confusion matrix in machine learning, and how can it be used to identify\n",
    "strengths and weaknesses of a model?\n",
    "ans-A confusion matrix, also known as an error matrix, is a table that is used to summarize the performance of a classification model. It presents the predicted labels of the model compared to the true labels of the data. The purpose of a confusion matrix is to provide a comprehensive overview of the model's predictions and to assess its strengths and weaknesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80b694e-12f9-4c8e-90cf-b8f58e3c9db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. What are some common intrinsic measures used to evaluate the performance of unsupervised\n",
    "learning algorithms, and how can they be interpreted?\n",
    "ans-When evaluating the performance of unsupervised learning algorithms, there are several common intrinsic measures that can be used. These measures provide insights into the quality of the learned representations or clusters without relying on external labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabbb6f0-de8a-4060-9d15-cb1daf99dc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. What are some limitations of using accuracy as a sole evaluation metric for classification tasks, and\n",
    "how can these limitations be addressed?\n",
    "ans-Using accuracy as the sole evaluation metric for classification tasks has some limitations, and it may not provide a complete picture of a model's performance. Here are some limitations of accuracy and how they can be addressed:\n",
    "\n",
    "Imbalanced Datasets: Accuracy can be misleading when dealing with imbalanced datasets, where the number of samples in different classes is significantly different. In such cases, a model that predicts the majority class most of the time can achieve high accuracy, but it may not perform well on the minority class. To address this, other evaluation metrics such as precision, recall, F1 score, or area under the receiver operating characteristic curve (AUC-ROC) can be used, which take into account both true positive and false positive rates."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
