{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8fec4a-35ec-47f0-8254-74f8873c3ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the KNN algorithm?\n",
    "ans-It is a non-parametric and instance-based learning algorithm, meaning it doesn't make assumptions about the underlying data distribution and makes predictions based on the instances (data points) in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca675d1-8c5d-41d8-bd89-145d6b848e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. How do you choose the value of K in KNN?\n",
    "ans-the optimal value of k may vary depending on the specific dataset and problem at hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97921e0-d516-4d29-8e49-274f5e59e098",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "ans-the KNN classifier is used for classification problems with discrete class labels, while the KNN regressor is used for regression problems with continuous target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de09e4db-d54d-445d-96db-e8c9e6e153fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. How do you measure the performance of KNN?\n",
    "ans-Confusion metric parameters like accuracy, recall,precision and F1 score is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de1be98-2670-42f7-b5fe-6d22cdb018aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. What is the curse of dimensionality in KNN?\n",
    "ans-The \"curse of dimensionality\" refers to a phenomenon that occurs in high-dimensional spaces, where the performance of certain algorithms, including KNN (k-nearest neighbors), degrades as the number of dimensions increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca3f2c6-2623-48c1-a7e7-8af524c4dc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. How do you handle missing values in KNN?\n",
    "ans-The imputation technique should be selected carefully, taking into consideration the potential biases and assumptions introduced during the imputation process. Additionally, it is crucial to evaluate the impact of the imputation on the performance of the KNN algorithm through appropriate validation techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e65e778-91b6-4f19-8cd5-fe635cc06025",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?\n",
    "ans-In general, the choice between the KNN classifier and regressor depends on the nature of the problem and the desired output. If the problem involves predicting discrete class labels, the KNN classifier is appropriate. On the other hand, if the problem involves predicting continuous target values, the KNN regressor is suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed98636-83e0-4eb9-94ed-c3e9033b3eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?\n",
    "ans-Strengths of KNN:\n",
    "\n",
    "Simplicity: KNN is a simple and intuitive algorithm that is easy to understand and implement.\n",
    "Non-linearity: KNN can handle non-linear relationships between features and the target variable without assuming any specific functional form.\n",
    "No Training Phase: KNN is a lazy learning algorithm, which means it does not require an explicit training phase. The entire training dataset is used during the prediction phase.\n",
    "Flexibility: KNN can work with various types of data, including categorical and numerical features.\n",
    "Weaknesses of KNN:\n",
    "\n",
    "Computational Complexity: The computational complexity of KNN grows with the number of training instances, making it less efficient for large datasets. Additionally, distance calculations in high-dimensional spaces can be computationally expensive.\n",
    "Sensitivity to Feature Scaling: KNN is sensitive to the scale of features. Features with larger scales can dominate the distance calculations, leading to biased predictions. Feature scaling, such as normalization or standardization, is often necessary.\n",
    "Curse of Dimensionality: KNN performance deteriorates as the number of dimensions increases. In high-dimensional spaces, data sparsity increases, and the notion of distance becomes less reliable, resulting in less accurate predictions. Dimensionality reduction techniques and careful feature selection can help mitigate this issue.\n",
    "Imbalanced Data: KNN can struggle with imbalanced datasets, where one class is significantly more prevalent than others. The majority class can dominate the predictions, resulting in biased results. Techniques like oversampling, undersampling, or using weighted distances can address this problem.\n",
    "Missing Data Handling: KNN does not handle missing values inherently, requiring additional preprocessing steps such as imputation or deletion of instances/features with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3152b70-fb1d-48b4-b0a8-4d2dbfda6a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "ans-Euclidean distance is calculated as the straight-line distance between two points in Euclidean space.\n",
    "Manhattan distance is calculated as the sum of absolute differences between the coordinates of two points in each dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b29b54-cf46-443e-8973-60d834bd2469",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10. What is the role of feature scaling in KNN?\n",
    "ans-Feature scaling plays a crucial role in KNN (k-nearest neighbors) algorithm. It is the process of normalizing or standardizing the feature values to ensure that all features contribute equally to the distance calculations. The main role of feature scaling in KNN is to address the issue of disproportionate influence of features with larger scales on the distance measurements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
